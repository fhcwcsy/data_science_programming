delId = which(stopLine < 1e-03)
tdm = tdm[-delId,]
tfidfMat = tfidfMat[-delId,]
# rank
rank = function(col) {rownames(tfidfMat)[order(-col)][1:30]}
rank = apply( tfidfMat, 2, rank )
rank
tdm = tdm_raw
n = length(tdm)
print(n)
tf = apply(as.matrix(tdm[,3:n]), 2, sum) #total number of words in every documents
idfCal = function(doc)
{
log2( (length(doc)) / Matrix::nnzero(doc) )
}
idf = apply(as.matrix(tdm[,3:n]), 1, idfCal)
tfidfMat = as.matrix(tdm[,2:n])
tfidfCal1 = function(col)
{
as.numeric(col) * idf
}
tfidfCal2 = function(row)
{
row / tf
}
wordscol = tfidfMat[, 1]
tfidfMat = apply(tfidfMat[, 2:ncol(tfidfMat)], 2, tfidfCal1)
tfidfMat = apply(tfidfMat, 1, tfidfCal2)
tfidfMat = t(tfidfMat)
rownames( tfidfMat) = wordscol
head(tfidfMat)
# remove trash words
stopLine = rowSums(tfidfMat)
delId = which(stopLine < 1e-01)
tdm = tdm[-delId,]
tfidfMat = tfidfMat[-delId,]
# rank
rank = function(col) {rownames(tfidfMat)[order(-col)][1:30]}
rank = apply( tfidfMat, 2, rank )
rank
tdm = tdm_raw
n = length(tdm)
print(n)
tf = apply(as.matrix(tdm[,3:n]), 2, sum) #total number of words in every documents
idfCal = function(doc)
{
log2( (length(doc)) / Matrix::nnzero(doc) )
}
idf = apply(as.matrix(tdm[,3:n]), 1, idfCal)
tfidfMat = as.matrix(tdm[,2:n])
tfidfCal1 = function(col)
{
as.numeric(col) * idf
}
tfidfCal2 = function(row)
{
row / tf
}
wordscol = tfidfMat[, 1]
tfidfMat = apply(tfidfMat[, 2:ncol(tfidfMat)], 2, tfidfCal1)
tfidfMat = apply(tfidfMat, 1, tfidfCal2)
tfidfMat = t(tfidfMat)
rownames( tfidfMat) = wordscol
head(tfidfMat)
# remove trash words
stopLine = rowSums(tfidfMat)
delId = which(stopLine == 0)
tdm = tdm[-delId,]
tfidfMat = tfidfMat[-delId,]
# rank
rank = function(col) {rownames(tfidfMat)[order(-col)][1:30]}
rank = apply( tfidfMat, 2, rank )
rank
tdm = tdm_raw
n = length(tdm)
print(n)
tf = apply(as.matrix(tdm[,3:n]), 2, sum) #total number of words in every documents
idfCal = function(doc)
{
log2( (length(doc) + 1) / Matrix::nnzero(doc) )
}
idf = apply(as.matrix(tdm[,3:n]), 1, idfCal)
tfidfMat = as.matrix(tdm[,2:n])
tfidfCal1 = function(col)
{
as.numeric(col) * idf
}
tfidfCal2 = function(row)
{
row / tf
}
wordscol = tfidfMat[, 1]
tfidfMat = apply(tfidfMat[, 2:ncol(tfidfMat)], 2, tfidfCal1)
tfidfMat = apply(tfidfMat, 1, tfidfCal2)
tfidfMat = t(tfidfMat)
rownames( tfidfMat) = wordscol
head(tfidfMat)
# remove trash words
stopLine = rowSums(tfidfMat)
delId = which(stopLine == 0)
tdm = tdm[-delId,]
tfidfMat = tfidfMat[-delId,]
# rank
rank = function(col) {rownames(tfidfMat)[order(-col)][1:30]}
rank = apply( tfidfMat, 2, rank )
rank
tdm = tdm_raw
n = length(tdm)
print(n)
tf = apply(as.matrix(tdm[,3:n]), 2, sum) #total number of words in every documents
idfCal = function(doc)
{
log2( (length(doc) ) / Matrix::nnzero(doc) )
}
idf = apply(as.matrix(tdm[,3:n]), 1, idfCal)
tfidfMat = as.matrix(tdm[,2:n])
tfidfCal1 = function(col)
{
as.numeric(col) * idf
}
tfidfCal2 = function(row)
{
row / tf
}
wordscol = tfidfMat[, 1]
tfidfMat = apply(tfidfMat[, 2:ncol(tfidfMat)], 2, tfidfCal1)
tfidfMat = apply(tfidfMat, 1, tfidfCal2)
tfidfMat = t(tfidfMat)
rownames( tfidfMat) = wordscol
head(tfidfMat)
# remove trash words
stopLine = rowSums(tfidfMat)
delId = which(stopLine == 0)
tdm = tdm[-delId,]
tfidfMat = tfidfMat[-delId,]
# rank
rank = function(col) {rownames(tfidfMat)[order(-col)][1:30]}
rank = apply( tfidfMat, 2, rank )
rank
gsub(X, "", colnames(tfidfMat))
gsub("X", "", colnames(tfidfMat))
# remove trash words
stopLine = rowSums(tfidfMat)
delId = which(stopLine == 0)
tdm = tdm[-delId,]
tfidfMat = tfidfMat[-delId,]
colnames(tfidfMat) = gsub("X", "", colnames(tfidfMat))
# rank
rank = function(col) {rownames(tfidfMat)[order(-col)][1:30]}
rank = apply( tfidfMat, 2, rank )
rank
source('~/Documents/summerproj/data_science_programming/week_2/day2/tfidf.R', echo=TRUE)
head(tdm)
colnames(tdm) = gsub("X", "", colnames(tdm))
head(tdm)
tail(tdm)
write.csv(rank, file = "./rank.csv")
source('~/Documents/summerproj/data_science_programming/week_2/day2/tfidf.R', echo=TRUE)
write.csv(tfidfMat, file = "./tfidf.csv")
source('~/Documents/summerproj/data_science_programming/week_2/day2/plot.R', echo=TRUE)
fileName = gsub( "X", "", colnames(rank_raw) )
filename = paste0( "./", filename, ".txt")
filename
fileName = gsub( "X", "", colnames(rank_raw) )
fileName = paste0( "./", fileName, ".txt")
fileName
fileName = gsub( "X", "", colnames(rank_raw)[2:ncol(rank)] )
fileName = paste0( "./", fileName, ".txt")
fileName
fileName = paste0( "./", fileName, ".txt")
size = apply(fileName, 1, fileSize(unit="kB"))
size = apply(fileName, 1, file.size(unit="kB"))
size = apply(fileName, 1, file.size)
fileName = gsub( "X", "", colnames(rank_raw)[2:ncol(rank)] )
fileName = paste0( "./", fileName, ".txt")
size = apply(fileName, 1, file.size)
fileName
fileName = gsub( "X", "", colnames(rank_raw)[2:ncol(rank)] )
fileName = paste0( "./data/", fileName, ".txt")
size = apply(fileName, 1, file.size)
fileName
class(fileName)
dim(fileName)
size = sapply(fileName, file.size)
size
yearname = gsub( "X", "", colnames(rank_raw)[2:ncol(rank)] )
fileName = paste0( "./data/", yearname, ".txt")
size = sapply(fileName, file.size)
colnames(size) = yearname
class(size)
yearname = gsub( "X", "", colnames(rank_raw)[2:ncol(rank)] )
fileName = paste0( "./data/", yearname, ".txt")
size = sapply(fileName, file.size)
sizedf = data.frame(yearname, size)
str(sizedf)
sizedf
year = gsub( "X", "", colnames(rank_raw)[2:ncol(rank)] )
fileName = paste0( "./data/", year, ".txt")
size = sapply(fileName, file.size)
sizedf = data.frame(year, size)
str(sizedf)
ggplot(sizedf, aes(x = year, y = size)) + geom_bar(stat="identity")
year = gsub( "X", "", colnames(rank_raw)[2:ncol(rank)] )
fileName = paste0( "./data/", year, ".txt")
size_B = sapply(fileName, file.size)
sizedf = data.frame(year, size_B)
ggplot(sizedf, aes(x = year, y = size_B)) + geom_bar(stat="identity")
tdm_raw
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
head(tdm)
colnames(tdm) = c("word", year)
head(tdm)
year
head(rank_raw)
#total amount of posts by year
year = gsub( "X", "", colnames(rank_raw)[2:(ncol(rank)+1)] )
fileName = paste0( "./data/", year, ".txt")
size_B = sapply(fileName, file.size)
sizedf = data.frame(year, size_B)
ggplot(sizedf, aes(x = year, y = size_B)) + geom_bar(stat="identity")
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
tail(tdm, 20)
#total amount of posts by year
year = gsub( "X", "", colnames(rank_raw)[2:(ncol(rank)+1)] )
fileName = paste0( "./data/", year, ".txt")
size_B = sapply(fileName, file.size)
sizedf = data.frame(year, size_B)
ggplot(sizedf, aes(x = year, y = size_B)) + geom_bar(stat="identity")
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
head(tdm)
tdm = arrange(tdm, desc(2019))
tdm = arrange(tdm, desc("2019"))
class(tdm)
colnames(tdm) = as.character(c("word", year))
tdm = arrange(tdm, desc("2019"))
head(tdm)
tdm = arrange(tdm, desc("word"))
colnames(tdm)
tdm = arrange(tdm, desc("2019"))
someyear = "2019"
tdm = arrange(tdm, desc(someyear))
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc("2019"))
class(tdm[3,3])
head(tdm)
tdm = arrange(tdm, desc(2019))
tdm = arrange(tdm, desc(tdm[,'2019']))
head(tdm)
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(tdm[,'word'], keepNA = 1) != 1)
class(tdm[,'word'])
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(as.character(tdm[,'word']), keepNA = 1) != 1)
head(tdm, 10)
#find the trends of the top words this year
tfidf = select(tfidf_raw, 3:ncol(tfidf_raw))
colnames(tfidf) = c("word", year)
tfidf = arrange(tfidf, desc(tfidf[,'2019'])) %>%
filter( nchar(as.character(tfidf[,'word']), keepNA = 1) != 1)
head(tfidf_raw)
#find the trends of the top words this year
tfidf = select(tfidf_raw, 1:ncol(tfidf_raw))
colnames(tfidf) = c("word", year)
tfidf = arrange(tfidf, desc(tfidf[,'2019'])) %>%
filter( nchar(as.character(tfidf[,'word']), keepNA = 1) != 1)
#find the trends of the top words this year
tfidf = select(tfidf_raw, 1:ncol(tfidf_raw))
colnames(tfidf) = c("word", year)
tfidf = arrange(tfidf, desc(tfidf[,'2019'])) %>%
filter( nchar(as.character(tfidf[,'word']), keepNA = 1) != 1)
head(tfidf)
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(as.character(tdm[,'word']), keepNA = 1) != 1)
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(as.character(tdm[,'word']), keepNA = 1) != 1)
head(tdm)
tdm = arrange(tdm, desc(tdm[,'2019']))
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc(tdm[,'2019']))
head(tdm)
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(tdm[,'word'], keepNA = 1) != 1)
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(as.character(tdm[,'word']), keepNA = 1) != 1)
head(tdm)
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(as.character(tdm[,'word'])) != 1)
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(as.character(tdm[,'word'])) != 1)
head(tdm)
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(as.character(word)) != 1)
head(tdm)
size_B
word_tmp = tdm[,1]
tdm_normalized = apply(tdm[,2:(ncol(tdm))], 1, normalize)
normalize = function(row)
{
row / size_B
}
word_tmp = tdm[,1]
tdm_normalized = apply(tdm[,2:(ncol(tdm))], 1, normalize)
rownames = word_tmp
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(as.character(word)) != 1)
head(tdm)
normalize = function(row)
{
row / size_B
}
word_tmp = tdm[,1]
tdm_normalized = apply(tdm[,2:(ncol(tdm))], 1, normalize)
rownames(tdm_normalized) = word_tmp
nrow(tdm_normalized)
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(as.character(word)) != 1)
head(tdm)
normalize = function(row)
{
row / size_B
}
word_tmp = tdm[,1]
tdm_normalized = t(apply(tdm[,2:(ncol(tdm))], 1, normalize))
head(tdm_normalized)
rownames(tdm_normalized) = word_tmp
head(tdm_normalized)
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(as.character(word)) != 1)
head(tdm)
normalize = function(row)
{
row / size_B
}
word_tmp = tdm[,1]
tdm_normalized = t(apply(tdm[,2:(ncol(tdm))], 1, normalize))
rownames(tdm_normalized) = word_tmp
head(tdm_normalized)
tdm_test = arrange(tdm, desc(tdm[,'2019']))
head(tdm_test)
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(as.character(word)) != 1)
head(tdm)
normalize = function(row)
{
row / size_B
}
word_tmp = tdm[,1]
tdm_normalized = t(apply(tdm[,2:(ncol(tdm))], 1, normalize))
rownames(tdm_normalized) = word_tmp
head(tdm_normalized)
tdm_test = arrange(tdm_normalized, desc(tdm_normalized[,'2019']))
head(tdm_test)
class(tdm_normalized[5,5])
#total amount of posts by year
year = gsub( "X", "", colnames(rank_raw)[2:(ncol(rank)+1)] )
fileName = paste0( "./data/", year, ".txt")
size_MB = sapply(fileName, file.size) /(1024^2)
sizedf = data.frame(year, size_MB)
ggplot(sizedf, aes(x = year, y = size_MB)) + geom_bar(stat="identity")
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(as.character(word)) != 1)
head(tdm)
normalize = function(row)
{
row / size_B
}
word_tmp = tdm[,1]
tdm_normalized = t(apply(tdm[,2:(ncol(tdm))], 1, normalize))
rownames(tdm_normalized) = word_tmp
head(tdm_normalized)
#total amount of posts by year
year = gsub( "X", "", colnames(rank_raw)[2:(ncol(rank)+1)] )
fileName = paste0( "./data/", year, ".txt")
size_MB = sapply(fileName, file.size) /(1024^2)
sizedf = data.frame(year, size_MB)
ggplot(sizedf, aes(x = year, y = size_MB)) + geom_bar(stat="identity")
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(as.character(word)) != 1)
head(tdm)
normalize = function(row)
{
row / size_MB * 1000
}
word_tmp = tdm[,1]
tdm_normalized = t(apply(tdm[,2:(ncol(tdm))], 1, normalize))
rownames(tdm_normalized) = word_tmp
head(tdm_normalized)
topten = tdm_normalized[1:10,]
topten
ggplot(topten, aes( year, freq)) + geom_line(aes(color = word))
class(topten)
topten = data.frame(word_tmp[1:10], tdm_normalized[1:10,] )
ggplot(topten, aes( year, freq)) + geom_line(aes(color = word))
ggplot(topten, aes( year, freq)) + geom_line(aes(color = word_tmp))
topten_melted = melt(t(topten))
library(reshape2)
topten_melted = melt(t(topten))
head(topten_melted, 15)
topten = data.frame(word_tmp[1:10], tdm_normalized[1:10,] )
head(topten, 15)
library(reshape2)
topten_melted = melt(t(topten))
topten = tdm_normalized[1:10,]
head(topten, 15)
library(reshape2)
topten_melted = melt(t(topten))
head(topten_melted)
colnames(topten_melted) = c(year, word, freq)
head(topten_melted)
ncol(topten_melted)
topten = tdm_normalized[1:10,]
head(topten, 15)
library(reshape2)
topten_melted = melt(t(topten))
colnames(topten_melted) = c("year", "word", "freq")
head(topten_melted)
ggplot(topten, aes( year, freq)) + geom_line(aes(color = word))
ggplot(topten_melted, aes( year, freq)) + geom_line(aes(color = word))
ggplot(topten_melted, aes( year, freq)) + geom_line(aes(color = word, shape = word))
ggplot(topten_melted, aes( year, freq)) + geom_line(aes(color = word)), geom_point(aes(color = word, shape = word))
ggplot(topten_melted, aes( year, freq)) + geom_line(aes(color = word)) + geom_point(aes(color = word, shape = word))
ggplot(topten_melted, aes( year, freq)) + geom_line(aes(color = word))
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(as.character(word)) != 1 & as.character(word) != "網址")
#total amount of posts by year
year = gsub( "X", "", colnames(rank_raw)[2:(ncol(rank)+1)] )
fileName = paste0( "./data/", year, ".txt")
size_MB = sapply(fileName, file.size) /(1024^2)
sizedf = data.frame(year, size_MB)
ggplot(sizedf, aes(x = year, y = size_MB)) + geom_bar(stat="identity")
#find the trends of the top words this year
tdm = select(tdm_raw, 3:ncol(tdm_raw))
colnames(tdm) = c("word", year)
tdm = arrange(tdm, desc(tdm[,'2019'])) %>%
filter( nchar(as.character(word)) != 1 & as.character(word) != "網址")
head(tdm)
normalize = function(row)
{
row / size_MB * 1000
}
word_tmp = tdm[,1]
tdm_normalized = t(apply(tdm[,2:(ncol(tdm))], 1, normalize))
rownames(tdm_normalized) = word_tmp
head(tdm_normalized)
topten = tdm_normalized[1:10,]
head(topten, 15)
library(reshape2)
topten_melted = melt(t(topten))
colnames(topten_melted) = c("year", "word", "freq")
head(topten_melted)
ggplot(topten_melted, aes( year, freq)) + geom_line(aes(color = word))
topten = tdm_normalized[1:10,]
head(topten, 15)
library(reshape2)
topten_melted = melt(t(topten))
colnames(topten_melted) = c("year", "word", "normalized_frequency")
head(topten_melted)
ggplot(topten_melted, aes( year, freq)) + geom_line(aes(color = word))
colnames(topten_melted) = c("year", "word", "normalized_frequency")
head(topten_melted)
ggplot(topten_melted, aes( year, normalized_frequency)) + geom_line(aes(color = word))
kable(topten)
library(dplyr)
readChar("./data/2018.txt", file.info("./data/2018.txt")$size) %>%
head
